"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ÙƒØ´Ù Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ
Enhanced Detection Engine with Self-Learning
"""

import re
import json
import logging
from datetime import datetime, timedelta
from typing import Tuple, List, Dict
from difflib import SequenceMatcher
from app.models.init_db import SessionLocal, Keyword, DeletedMessage

logger = logging.getLogger(__name__)


class EnhancedDetectionEngine:
    """Ù…Ø­Ø±Ùƒ ÙƒØ´Ù Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ"""
        self.base_keywords = {
            # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
            'Ø¥Ø¬Ø§Ø²Ø© Ù…Ø±Ø¶ÙŠØ©': 0.95,
            'Ù…Ø±Ø¶ÙŠ': 0.9,
            'Ø·Ø¨ÙŠ': 0.85,
            'Ø¹ÙŠØ§Ø¯Ø©': 0.8,
            'Ù…Ø³ØªØ´ÙÙ‰': 0.85,
            'Ø¯ÙƒØªÙˆØ±': 0.7,
            'Ø·Ø¨ÙŠØ¨': 0.7,
            'Ø¹Ù„Ø§Ø¬': 0.7,
            'Ø¯ÙˆØ§Ø¡': 0.65,
            'ØªÙ‚Ø±ÙŠØ± Ø·Ø¨ÙŠ': 0.95,
            'Ø´Ù‡Ø§Ø¯Ø© Ø·Ø¨ÙŠØ©': 0.95,
            'ÙØ­Øµ Ø·Ø¨ÙŠ': 0.9,
            'Ù…ÙˆØ¹Ø¯ Ø·Ø¨ÙŠ': 0.85,
            'Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯': 0.75,
            'Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ©': 0.85,
            
            # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ØºÙŠØ§Ø¨
            'ØºÙŠØ§Ø¨': 0.8,
            'Ø¹Ø·Ù„Ø©': 0.7,
            'Ø¥Ø¬Ø§Ø²Ø©': 0.75,
            'Ø¹Ø¯Ù… Ø§Ù„Ø­Ø¶ÙˆØ±': 0.85,
            'Ø¹Ø°Ø±': 0.7,
            'ØªØºÙŠØ¨': 0.8,
            
            # ÙƒÙ„Ù…Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©
            'Ù…ÙˆØ«ÙˆÙ‚': 0.8,
            'Ù…Ø¹ØªÙ…Ø¯': 0.75,
            'Ø­ÙƒÙˆÙ…ÙŠ': 0.7,
            'Ø±Ø³Ù…ÙŠ': 0.7,
            'Ø´Ù‡Ø§Ø¯Ø©': 0.8,
            'ØªÙ‚Ø±ÙŠØ±': 0.75,
        }
        
        # Ø£Ù†Ù…Ø§Ø· regex Ù„Ù„ÙƒØ´Ù Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        self.patterns = {
            'phone': re.compile(r'(\+\d{1,3}[-.\s]?)?\d{3,4}[-.\s]?\d{3,4}[-.\s]?\d{4}'),
            'email': re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'),
            'url': re.compile(r'https?://[^\s]+|www\.[^\s]+'),
            'numbers': re.compile(r'\d{7,}'),  # Ø£Ø±Ù‚Ø§Ù… Ø·ÙˆÙŠÙ„Ø©
            'special_chars': re.compile(r'[^\w\s\u0600-\u06FF]'),
        }
        
        # Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„Ø°Ø§ØªÙŠ
        self.learning_data = {
            'detected_keywords': {},
            'false_positives': [],
            'false_negatives': [],
            'improvement_history': []
        }
        
        self.load_learning_data()
    
    def load_learning_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        try:
            import os
            learning_file = os.path.join(
                os.path.dirname(__file__),
                '../../data/learning_data.json'
            )
            if os.path.exists(learning_file):
                with open(learning_file, 'r', encoding='utf-8') as f:
                    self.learning_data = json.load(f)
                logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ")
        except Exception as e:
            logger.warning(f"ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù…: {e}")
    
    def save_learning_data(self):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„Ù"""
        try:
            import os
            data_dir = os.path.join(os.path.dirname(__file__), '../../data')
            os.makedirs(data_dir, exist_ok=True)
            
            learning_file = os.path.join(data_dir, 'learning_data.json')
            with open(learning_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_data, f, ensure_ascii=False, indent=2)
            logger.info("âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ")
        except Exception as e:
            logger.warning(f"ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¹Ù„Ù…: {e}")
    
    def normalize_text(self, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø¨Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text = re.sub(r'[\u064B-\u0652]', '', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'\s+', ' ', text)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©
        text = text.lower().strip()
        
        return text
    
    def extract_keywords(self, text: str) -> List[Tuple[str, float]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        normalized = self.normalize_text(text)
        found_keywords = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        for keyword, score in self.base_keywords.items():
            if keyword in normalized:
                found_keywords.append((keyword, score))
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
                if keyword not in self.learning_data['detected_keywords']:
                    self.learning_data['detected_keywords'][keyword] = 0
                self.learning_data['detected_keywords'][keyword] += 1
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© (fuzzy matching)
        words = normalized.split()
        for word in words:
            if len(word) > 3:
                for keyword in self.base_keywords.keys():
                    similarity = SequenceMatcher(None, word, keyword).ratio()
                    if 0.75 < similarity < 1.0:  # ÙƒÙ„Ù…Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù„ÙƒÙ† Ù„ÙŠØ³Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø©
                        score = self.base_keywords[keyword] * similarity * 0.8
                        found_keywords.append((keyword, score))
        
        return found_keywords
    
    def calculate_obfuscation_score(self, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙ…ÙˆÙŠÙ‡ (ÙƒÙ… Ù…Ø±Ø© Ø­Ø§ÙˆÙ„ Ø§Ù„Ù…Ø±Ø³Ù„ Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©)"""
        score = 0.0
        
        # ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø§ÙØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ø£Ø­Ø±Ù
        if re.search(r'\w\s+\w', text):
            score += 0.15
        
        # ÙˆØ¬ÙˆØ¯ Ø£Ø­Ø±Ù Ø®Ø§ØµØ© ÙƒØ«ÙŠØ±Ø©
        special_count = len(re.findall(self.patterns['special_chars'], text))
        if special_count > len(text) * 0.2:
            score += 0.2
        
        # ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù…
        if re.search(self.patterns['numbers'], text):
            score += 0.15
        
        # ÙˆØ¬ÙˆØ¯ Ø±ÙˆØ§Ø¨Ø· Ø£Ùˆ Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
        if re.search(self.patterns['url'], text) or re.search(self.patterns['email'], text):
            score += 0.25
        
        # ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù… Ù‡ÙˆØ§ØªÙ
        if re.search(self.patterns['phone'], text):
            score += 0.25
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        if text and len(text) > 0:
            upper_ratio = sum(1 for c in text if c.isupper()) / len(text)
            if upper_ratio > 0.5:
                score += 0.1
        
        return min(score, 1.0)
    
    def detect_spam(
        self,
        text: str,
        user_id: int = None,
        chat_id: int = None,
        sensitivity: float = 0.7
    ) -> Tuple[bool, float, List[str]]:
        """
        ÙƒØ´Ù Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© Ù…Ø¹ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ
        
        Returns:
            (is_spam, confidence, keywords)
        """
        if not text or len(text.strip()) == 0:
            return False, 0.0, []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = self.extract_keywords(text)
        
        if not keywords:
            return False, 0.0, []
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keyword_score = max([score for _, score in keywords]) if keywords else 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙ…ÙˆÙŠÙ‡
        obfuscation_score = self.calculate_obfuscation_score(text)
        
        # Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© = Ù…ØªÙˆØ³Ø· Ø¯Ø±Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª + Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙ…ÙˆÙŠÙ‡
        final_score = (keyword_score * 0.7) + (obfuscation_score * 0.3)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        threshold = 1.0 - sensitivity
        is_spam = final_score >= threshold
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙ‚Ø·
        keyword_names = [kw for kw, _ in keywords]
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ
        if is_spam:
            self.learning_data['improvement_history'].append({
                'timestamp': datetime.now().isoformat(),
                'text': text[:50],
                'score': final_score,
                'keywords': keyword_names
            })
        
        return is_spam, final_score, keyword_names
    
    def add_false_positive(self, text: str, keywords: List[str]):
        """ØªØ³Ø¬ÙŠÙ„ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø®Ø§Ø·Ø¦ (Ø±Ø³Ø§Ù„Ø© ØªÙ… Ø­Ø°ÙÙ‡Ø§ Ø¨Ø§Ù„Ø®Ø·Ø£)"""
        self.learning_data['false_positives'].append({
            'timestamp': datetime.now().isoformat(),
            'text': text[:100],
            'keywords': keywords
        })
        
        # ØªÙ‚Ù„ÙŠÙ„ Ø¯Ø±Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©
        for keyword in keywords:
            if keyword in self.base_keywords:
                self.base_keywords[keyword] *= 0.95
        
        logger.info(f"ðŸ“ ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ø®Ø§Ø·Ø¦: {keywords}")
        self.save_learning_data()
    
    def add_false_negative(self, text: str, keywords: List[str]):
        """ØªØ³Ø¬ÙŠÙ„ Ø³Ù„Ø¨ÙŠ Ø®Ø§Ø·Ø¦ (Ø±Ø³Ø§Ù„Ø© Ù…Ø²Ø¹Ø¬Ø© Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§)"""
        self.learning_data['false_negatives'].append({
            'timestamp': datetime.now().isoformat(),
            'text': text[:100],
            'keywords': keywords
        })
        
        # Ø²ÙŠØ§Ø¯Ø© Ø¯Ø±Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
        for keyword in keywords:
            if keyword in self.base_keywords:
                self.base_keywords[keyword] = min(self.base_keywords[keyword] * 1.05, 1.0)
        
        logger.info(f"ðŸ“ ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø³Ù„Ø¨ÙŠ Ø®Ø§Ø·Ø¦: {keywords}")
        self.save_learning_data()
    
    def get_learning_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ"""
        return {
            'total_detections': len(self.learning_data['improvement_history']),
            'false_positives': len(self.learning_data['false_positives']),
            'false_negatives': len(self.learning_data['false_negatives']),
            'top_keywords': sorted(
                self.learning_data['detected_keywords'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5],
            'accuracy': self._calculate_accuracy()
        }
    
    def _calculate_accuracy(self) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ÙƒØ´Ù"""
        total = (
            len(self.learning_data['improvement_history']) +
            len(self.learning_data['false_positives']) +
            len(self.learning_data['false_negatives'])
        )
        
        if total == 0:
            return 0.0
        
        correct = len(self.learning_data['improvement_history'])
        return (correct / total) * 100 if total > 0 else 0.0


# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ø­Ø±Ùƒ
detection_engine = EnhancedDetectionEngine()
